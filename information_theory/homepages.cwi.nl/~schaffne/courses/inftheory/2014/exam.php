<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
    <meta content="text/html; charset=ISO-8859-1" http-equiv="content-type">
    <link rel="stylesheet" href="../../static/courses.css" type="text/css">
    <link rel="shortcut icon" href="../../../static/img/favicon.ico" type="image/x-icon">

    <title>Information Theory - UvA course - Fall 2014</title>
</head>

<body>
<div id="content">
<h1>Information Theory 2014</h1>

<div id="topbox">
    <div class="subbox">
        <a href="http://www.uva.nl">University
            of Amsterdam course</a>, Fall 2014<br>
        <a href="http://www.illc.uva.nl/MScLogic/">Master of Logic</a>

        <div id="lecturer">
            Lecturer: <a href="http://homepages.cwi.nl/%7Eschaffne">Christian
                Schaffner</a> (<a href="http://www.uva.nl/">UvA</a> / <a
                href="http://homepages.cwi.nl/%7Eschaffne/contact.php">email</a>)
            <br>
            Teaching assistant: <a
                href="http://www.uva.nl/over-de-uva/organisatie/medewerkers/content/s/c/p.schulz/p.schulz.html">
                Philip Schulz</a> (<a href="mailto:P.Schulz@uva.nl">email</a>)
        </div>
    </div>

    <div class="subbox">
        Get back to <a href="index.php">the course site</a>.
    </div>

</div>

<!-- #topbox -->

<h2 id="start_of_text">General Procedure</h2>
For the final exam of the course, every student will pick an information-theoretic subject from the list below and
study
that.
Every student has to prepare and give a 20-minute talk followed by 10 minutes of questions about the chosen topic.


<h2>Detailed Procedure</h2>
<ul>
    <li>Look through the list of topic below. By Wednesday, 3 December, 13:00, send us by email
        on ordered list of your top 3 preferences. Note that you are welcome to add your own topic of choice, see
        below.
        Let us know as well if you
        have any constraints for the presentation slots during the exam week (Wed 17 Dec, Thu 18 Dec, all
        13:00-16:00).
    </li>
    <li>Study the material
        and think about how you want to present the subject to your
        fellow students in a 20-minute talk.
    <li>Write down in a few bullet points how you want to structure your talk. Send this list to Philip and Chris by
        <b>Wednesday, 10 December 2014, 13:00</b>.
    </li>
    <!--    <li>Prepare a written hand-out about the chosen topic, consisting of maximally two A4 pages. That hand-out will
            be handed out to the audience right before or after your talk (so you cannot assume the talk audience has
            studied your hand-out
            beforehand). Send a preliminary version of your handout and the talk structure to Christian and Malvin by
            <b>Wednesday, 15 October 2014, 14:00</b>.
            We will then use the lecture slot on Thursday, 16 October 2014 from 9:00-11:00 to give you individual feedback.
    -->
    <li>If you want, we can then arrange a personal meeting. In this meeting, we will discuss your talk outline and you
        can ask
        questions about the studied material
        (you are of course welcome to ask any questions at any time.)
        <!--    <li>Send us a PDF version of your handout until 11:00 of the day of your talk, we will print them and hand them out,-->
        <!--    the electronic version will be put on the course webpage.</li>-->
    <li>We will create the presentation schedule, you give a 20-minute
        talk, followed by 10 minutes of questions from the audience. (Yes, we will be
        asking questions.)
</ul>

<h3>Presentation</h3>
A presentation should <strong>make your fellow students care about the
    subject</strong> and <strong>convey the main ideas</strong> of the topic/paper. Try to show how your topic
connects
to something we have learned in class.
You should present the main result(s) with some precision, and it would be nice
to give some key ingredients of proofs (where appropriate - some proofs are nasty
and/or not interesting). Do not spend all your time putting up formula after
formula: prepare a presentation <em>you</em> would like to listen
to. It is important to stay on time, you leave a bad impression if your
20-minute talk lasts for 25 minutes or more.

Here are a few general tips:
<ul>
    <li>Keep it simple! The talk is only 20 minutes and the goal is really not to impress anybody with technical
        details,
        but instead to convey the main message to your fellow students who do not know anything about your topic.
    </li>
    <li>Use at least some slides, 20 minutes are probably not enough for a blackboard-only talk. Presenting from slides
        comes
        with the danger of going (way) too fast. Make sure everybody in the room can follow you.
        A rough average of 1min per slide tends to be reasonable.
    </li>
    <li>Practice your talk in order to get an estimate of the right timing!</li>
</ul>

<!--<h3 id="handout">Handout</h3>-->
<!--The handout consists of maximal 2 A4 pages giving an overview of the selected topic. A proper handout contains at least-->
<!--the title of the chosen subject, a general description of it, a high-level overview of the main ideas, and references-->
<!--to scientific articles and other resources you have used (please insert hyperlinks for very reference).-->
<!--The handout is <b>not</b> a means to help people understanding your talk.-->
<!--To the contrary, it is unrelated to your talk, and simply gives a quick two-page overview of the presented topic.-->
<!--Keep it accessible for the target audience of your fellow students. Make it look more attractive by putting one or more-->
<!--pictures or figures. Do not put too many formulas, it should be light to read and easy to digest.-->
<!--Send us a PDF version of your handout until 11:00 of the day of your talk, we will print them and hand them out.-->

<h3>Grades</h3>
Your grade for the final presentation will be determined by the
quality of the presentation, your ability to answer questions about
the subject (<b>we will use <a href="evaluation_forms.pdf">this list</a> for the evaluation</b>).
<!--and the quality of the hand-out. -->
<!--Your grade might be reduced if you do not follow the-->
<!--"detailed procedure" above. (This does not mean that -->
You do not have to stick to the
indicated sources. To the contrary, you are encouraged to find other articles or sources that are
relevant for the selected topic. The
final presentation counts 1/2 towards your final grade of the course
(the other 1/2 are determined by the homework exercises).

<!--<h2>Presentation tools</h2>
LaTeX has several presentation packages available: powerdot and beamer are
probably the best choices. (Powerdot is more modern, but both are quite
powerful.) Powerpoint and Apple's Keynote can be used too.
<p>
    Making slides is a lot of work, it usually takes me a few full days to
    prepare well for a 15-minute talk. Don't spend too much time on making it pretty before
    receiving our feedback.
-->

<h2>Research tools</h2>
Scientific papers can most easily be found online via Google or <a href="http://scholar.google.com">Google
    Scholar</a>. As an alternative, almost all universities have a
subscription to SpringerLink, allowing you to access these articles for free
from their computers. If neither works, feel free to contact us.


<h2>Suggested topics for final presentation:</h2>
Note that you are very welcome to suggest a topic yourself. In that case, send us (at least) one link to an academic
paper about the subject. We will let you know quickly if that subject is appropriate.

<p>
    Some subjects are far more challenging than others. <strike>The following list if roughly ordered in decreasing
        difficulty of topics.</strike>
    A good talk about a challenging subject is more impressive than a good talk about an easy subject,
    and the grade will reflect that.</p>

<p>Some topics are copied with kind permission from
    <a target="_blank"
       href="http://www.uva.nl/over-de-uva/organisatie/medewerkers/content/m/a/m.w.madsen/m.w.madsen.html">Mathias
        Madsen</a>'s
    <a target="_blank" href="http://informationtheory.weebly.com/presentation-topics.html">course on information
        theory</a></p>

<h3>Rate Distortion Theory</h3>

<p><a href="https://en.wikipedia.org/wiki/Rate%E2%80%93distortion_theory" target="_blank">Rate-distortion theory</a>
    gives an analytical expression for how much compression can be achieved using lossy compression
    methods; it addresses the problem of determining the minimal number of bits per symbol, as measured by the rate R,
    that
    should be communicated over a channel, so that the source (input signal) can be approximately reconstructed at the
    receiver (output signal) without exceeding a given distortion D.</p>

<p>Many of the existing audio, speech, image, and video compression techniques have transforms, quantization, and
    bit-rate
    allocation procedures that capitalize on the general shape of rate-distortion functions.</p>

<p>One of the most intriguing aspects of this theory is that joint descriptions are more efficient than individual
    descriptions. It is simpler to describe an elephant and a chicken with one description than to describe each alone.
    This
    is true even for independent random variables. It is simpler to describe \(X_1\) and \(X_2\) together (at a given
    distortion for
    each) than to describe each by itself. Why don't independent problems have independent solutions? The answer is
    found in
    the geometry. Apparently, rectangular grid points (arising from independent descriptions) do not fill up the space
    efficiently.</p>

<p>Study it in Section 10.4 in [MacKay] and Chapter 10 in [CT]</p>


<h3>Kolmogorov Komplexity</h3>

<p>The great mathematician <a href="https://en.wikipedia.org/wiki/Andrey_Kolmogorov" target="_blank">Kolmogorov</a>
    culminated a lifetime of research in mathematics, complexity, and information theory with his definition in
    1965 of the intrinsic descriptive complexity of an object. In the treatment in the course, the object \(X\) has been
    a random variable drawn according to a probability mass function \(P_X(x)\). If \(X\) is random, there is a sense in
    which the descriptive complexity of the event \(X = x\) is \(\log \frac{1}{P_X(x)}\) , because
    \(\lceil{\log \frac{1}{P_X(x)}}\rceil\) is the number of bits required to describe \(x\) by an optimal code.
    One notes immediately that the descriptive complexity of such an object depends on the probability distribution.</p>

<p>Kolmogorov went further. He defined the algorithmic (descriptive) complexity of an object to be the length of the
    shortest binary computer program that describes the object. (Apparently, a computer, the most general form of data
    decompressor, will after a finite amount of computation, use this description to exhibit the object described.)
    Thus,
    the Kolmogorov complexity of an object dispenses with the probability distribution. Kolmogorov made the crucial
    observation that the definition of complexity is essentially computer independent. It is an amazing fact that the
    expected length of the shortest binary computer description of a random variable is approximately equal to its
    entropy.
    Thus, the shortest computer description acts as a universal code which is uniformly good for all probability
    distributions. In this sense, algorithmic complexity is a conceptual precursor to entropy.</p>

<p>Learn more about the topic in Chapter 14 of [CT].</p>


<h3>Stopping Rules and Wald's equality</h3>
A <a href="http://en.wikipedia.org/wiki/Stopping_time" target="_blank">stopping rule</a> is a rule for deciding
when to stop doing something, based on a finite amount of past data - for
instance, when to leave the casino based on your past gambling record, or when to stop a series of clinical trials
based on the results so far. <a href="http://en.wikipedia.org/wiki/Wald%27s_equation" target="_blank">Wald's
    equality</a>
is a theorem which states that the average gain associated with a
specific stopping rule is equal to the expected gain from each individual trial, times the expected time before you
stop.

Some things you might want to do with this topic are:
<ul>
<li>Give a clear and careful proof of Wald's equality.</li>
<li>Discuss what the theorem entails for gambling. Is it bad news?</li>
<li>Discuss the implications of Wald's equality for statistics.</li>
<li>Consider the gambling strategy "Stop when you're ahead": Before you enter the casino, you decide to leave once you
have reached a certain level of capital c. What is the probability that you will ever leave the casino if you use
this stopping rule?</li>
<li>Think about the connection between stopping rules and Turing machines with infinite input tapes: A binary input tape
can be seen as path through a binary tree, and a Turing machine can be seen as a deterministic rule for deciding
whether you should stop or continue when you reach a particular node.</li>
<li>Consider the relation to finite stopping problems like the <a href="http://en.wikipedia.org/wiki/Secretary_problem"
                                                              target="_blank">
    secretary problem</a>.</li>
</ul>

<p>A good place to start learning about this topic is Robert Gallager's
<a href="http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-262-discrete-stochastic-processes-spring-2011/index.htm"
   target="_blank">MIT course on stochastic processes</a>.</p>


<h3>Entropy Rates of a Stochastic Process</h3>
The asymptotic equipartition property (AEP) established that \(n H(X)\) bits suffice on the average to describe \(n\)
iid random variables.
But what if the random variables are dependent? In particular, what if the random variables form a
<a href="https://en.wikipedia.org/wiki/Stationary_process" target="_blank">stationary process</a>?
In Chapter 4 of [CT], it is shown, just as in the iid case, that the entropy \(H(X_1,X_2, \ldots, X_n)\) grows
(asymptotically)
linearly in \(n\) at rate \(H(\mathcal{X})\), the entropy rate of the process.

<h3>Gambling and Data Compression</h3>
At first sight, information theory and gambling seem to be unrelated. But as you can read in Chapter 6 of [CT], there
is a strong duality between the growth rate of investment in a horse race and the entropy rate of the horse race.
Indeed,
the sum of the growth rate and the entropy rate is a constant. In the process of proving this, it is argued that the
financial value of side information is equal to the mutual information between the horse race and the side information.
In order to work on this topic, you will have to understand parts of the previous topic (Chapter 4 of [CT]) as well.


<h3>Codebreaking for Traditional Cipher Systems</h3>
Information theory provides general insights about the security of various encryption schemes. However, cracking an
"insecure" cipher often requires a lot of clever probabilistic inference, for instance in the form of sampling schemes
like Gibbs sampling of the posterior distribution over encryption keys.

Historically, cracking the German Enigma encryption machine was such a task, you can read about it in Section 18.4 of
[MacKay].
More recently, <a href="http://www.isi.edu/natural-language/people/knight.html" target="_blank">Kevin Knight</a>
and colleagues has also written a number of papers on codebreaking from an explicitly
Bayesian viewpoint (e.g., <a href="http://www.aclweb.org/anthology/W/W11/W11-1202.pdf" target="_blank">this one</a>).

If you like a programming challenge, you can also try to decipher the encrypted texts listed under this topic
on <a href="http://informationtheory.weebly.com/presentation-topics.html" target="_blank">Mathias' page</a> and report
on your findings.


<h3>Randomness Extraction and Privacy Amplifications</h3>
In cryptography, we want to limit the amount of information an eavesdropper learns. One way to make sure that a certain
n-bit string
X is close to unknown to an eavesdropper Eve who holds side information Y about X is to extract the randomness out of X
by picking a random function mapping n bits to less bits, thereby making it more difficult for Eve to learn f(X).
Study the details of this privacy-amplification procedure in Sections 8 and 9 of the [CF] script.


<h3>Hamming Codes</h3>
If you send messages through a binary channel with a fixed and symmetric probability of bit flips (i.e., 0 being
received as 1 or vice versa), then your task as an encoder is essentially to choose codewords that are as far apart
as
possible in terms of bit flip distance.

A method for creating such codebooks was invented by <a href="https://en.wikipedia.org/wiki/Hamming_code"
                                                        target="_blank">Hamming</a>
in the 1940s. It turns out that his encoding method can be
expressed as a certain kind of matrix multiplication, and the choice of encoding scheme for the binary symmetric
channel
thus corresponds to a choice of matrix. The reverse inference problem then similarly becomes a linear-algebra
problem.

Things you can do under this topic:
<ul>
    <li>Read Hamming's original paper and a more modern presentation of his ideas.</li>
    <li>Explain how the (7,4) Hamming code works.</li>
    <li>Explain the how Hamming codes can be represented as matrices.</li>
    <li>Find the error rate of a Hamming code and show it depends on the bit flip probability, the length of the
        code word, and
        the number of parity check bits.
    </li>
</ul>


<h3>Uniqueness of the Logarithmic Uncertainty Measure</h3>
In his 1948 paper, Shannon gave a small set of axioms that were sufficient to uniquely select entropy as a measure
of uncertainty (up to the choice of logarithmic base). Other authors since him have used slightly different but
equivalent sets of axioms, and many textbooks in information theory contain a version of this theorem.

Some things you can do under this topic are:
<ul>
    <li>Choose a specific set of axioms and prove that the entropy is the only function fulfilling them.</li>
    <li>Compare different axiomatizations.</li>
    <li>Discuss which of the axioms that constitute the most substantial assumptions, and if they can be relaxed.
    </li>
</ul>


<h3>Hash Codes: Codes for Efficient Information Retrieval</h3>
A simple example of an information-retrieval problem is the task of implementing a phone directory service, which, in
response to a person's name, returns (a) a confirmation that that person is listed in the directory; and (b) the
person's phone number and other details. This is a classical computer-science problem called the dictionary problem:
the task of designing a data structure that maintains a
set of data during 'search', 'delete' and 'insert' operations. A standard solution to the dictionary problem is a
<a href="https://en.wikipedia.org/wiki/Hash_table" target="_blank">hash table</a>. Learn all about it in Chapter 12 of
[MacKay].


<h3>Language Processing: Comprehension of different syntactic structures</h3>
<a target="_blank" href="http://idiom.ucsd.edu/~rlevy/papers/levy-2008-cognition.pdf">This paper</a> accounts for
language processing effects as witnessed through reading studies in information-theoretic terms. In particular, it uses
surprisal as a measure of processing load. The usefulness of this measure is corroborated by many experiments. It also
allows for a unified view on previous experimental findings which have traditionally been analyzed in different
theoretical frameworks. Finally, a suprisal-based theory does in principle not need to rely on structural assumptions
usually made by the generative school in linguistics. It therefore has the potential to free modern lingustic reasearch
of some of the theoretical burden carried by earlier approaches.


<h3>Entropy rates in language</h3>
<a target="_blank" href="http://dl.acm.org/citation.cfm?id=1073117">This paper</a> investigates the entropy rate of
English text. It was one of the first in a now blooming paradigm that tries to capture linguistic behaviour as
communication in an information-theoretic sense. The authors show that while the average entropy of each sentence in a
paragraph rises if the sentence is taken out of context, the entropy given all previous sentences stays more or less
constant. It is hypothesized that linguistic communication is organized so as communicate at a constant entropy rate.
Later research has backed up this hypothesis for spoken language. See also <a
target="_blank" href="http://www.sciencedirect.com/science/article/pii/S0010028510000083">this link</a>.


<h3>Machine Translation</h3>
<a target="_blank" href="http://www.aclweb.org/anthology/J93-2003">This</a> is the paper that started modern machine
translation. The ideas presented therein are still used in modern translation system, e.g. google translate. The
underlying idea is that a foreign language is basically just an encoded version of English that has to pass through a
noisy channel. The translation task then reduces to decoding the source message. The paper offers a sequence of
increasingly complex models to do the decoding. For the presentation, it will suffice to cover models 1 and 2. It may
also be interesting to compare it to Warren Weaver's vision of automated translation and check how much of it survived.


<h3>Maximum Entropy</h3>
The maximum entropy principle is one of the most successful and widely used ideas in machine learning. It basically
advocates using the model with the highest entropy. However, the choice of possible models is constrained by the data.
Put simply, the maximum entropy principle postualates "Model only what you know and nothing else". A presentation on
this topic may focus on either the maximum entropy principle itself
(Cover and Thomas, chapter 12) or on its applications (relevant papers can be obtained from the lecturers). Those are
widespread and found almost everywhere in modern machine learning systems. For example, the decision module in <a
target="_blank" href="http://www.youtube.com/watch?v=seNkjYyG3gI">IBM's Watson system</a> is a maximum entropy model. Also, many
neural network models can be viewed as a combination of maximum entropy models


<h3>Discriminative vs. Generative classifiers</h3>
Discriminative and generative models are the two main categories in which one can divide machine learning algorithms.
<a target="_blank" href="http://ai.stanford.edu/~ang/papers/nips01-discriminativegenerative.pdf">This paper</a> tries to compare them in
terms of their theoretical guarantees. It does so only for two of the most simple representatives of each class but has
spawned much follow-up research since is it crucial to know which kind of model to apply in which situation. This is one
of the more challenging topics and should only be taken
if you already have a background in machine learning.

</div>

<div id="footer">
    <p>Last update:  &nbsp; &nbsp;
        <span class="disclaimer"><a href="http://www.cwi.nl/disclaimer.html">CWI DISCLAIMER</a></span></p>


    <script type="text/javascript"
            src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>

    <!-- Start of StatCounter Code -->
    <script type="text/javascript" language="javascript">
        <!--
        var sc_project = 1639480;
        var sc_invisible = 1;
        var sc_partition = 15;
        var sc_security = "19c07acf";
        //-->
    </script>

    <script type="text/javascript" language="javascript" src="http://www.statcounter.com/counter/counter.js"></script>
    <noscript><a href="http://statcounter.com/" target="_blank"><img
                src="http://c16.statcounter.com/counter.php?sc_project=1639480&amp;java=0&amp;security=19c07acf&amp;invisible=1"
                alt="website page counter" border="0"></a></noscript>
    <!-- End of StatCounter Code -->

    <script type="text/javascript">

        var _gaq = _gaq || [];
        _gaq.push(['_setAccount', 'UA-23222525-1']);
        _gaq.push(['_trackPageview']);

        (function () {
            var ga = document.createElement('script');
            ga.type = 'text/javascript';
            ga.async = true;
            ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(ga, s);
        })();

    </script>

</div>

</body>
</html>
